{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCL0JGCbUwi"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Aa--cyvTWo_r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "from PIL import Image # Install Pillow -> conda install anaconda::pillow or pip install pillow\n",
        "import os\n",
        "from skimage.io import  imread, imshow # Install scikit-image -> conda install scikit-image or pip install scikit-image\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score,  mean_squared_error, r2_score, f1_score\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import numpy as np\n",
        "\n",
        "#import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Dropout\n",
        "import torch.optim as optim\n",
        "from torch.functional import F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA LOADING PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4vACqSHvkKo",
        "outputId": "cb383bf3-80d3-4aa1-f273-e6a5ff1642a0"
      },
      "outputs": [],
      "source": [
        "# Check if the code is running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    train_dataset_path = '/content/drive/MyDrive/Project1-AML/data-students/TRAIN'\n",
        "    test_dataset_path = '/content/drive/MyDrive/Project1-AML/data-students/TEST'\n",
        "else:\n",
        "    # Load data from local file\n",
        "    train_dataset_path = 'data-students/TRAIN'\n",
        "    test_dataset_path = 'data-students/TEST'\n",
        "\n",
        "# Now you can use file_path to load your data\n",
        "#print(\"File path:\", file_path)\n",
        "\n",
        "#FIXED VARIABLES\n",
        "IMG_WIDTH = 75 #75\n",
        "IMG_HEIGHT = 75 #75\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "#testing variables\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rembg import remove\n",
        "import io\n",
        "\n",
        "def remove_background_pil(image):\n",
        "    # Convert PIL image to bytes\n",
        "    with io.BytesIO() as output_buffer:\n",
        "        image.save(output_buffer, format='PNG')\n",
        "        image_bytes = output_buffer.getvalue()\n",
        "\n",
        "    # Use rembg to remove the background\n",
        "    result = remove(image_bytes)\n",
        "\n",
        "    # Convert the result binary data back to a PIL image\n",
        "    result_image = Image.open(io.BytesIO(result))\n",
        "\n",
        "    # Fill transparent pixels with black\n",
        "    result_image = fill_transparent_pixels_with_black(result_image)\n",
        "\n",
        "    # Convert the image to RGB mode if it's not already\n",
        "    if result_image.mode != 'RGB':\n",
        "        result_image = result_image.convert('RGB')\n",
        "        \n",
        "    return result_image\n",
        "\n",
        "def fill_transparent_pixels_with_black(image):\n",
        "    # Convert image to RGBA mode if it's not already\n",
        "    if image.mode != 'RGBA':\n",
        "        image = image.convert('RGBA')\n",
        "\n",
        "    # Get the image data as a pixel access object\n",
        "    pixel_data = image.load()\n",
        "\n",
        "    # Iterate over each pixel\n",
        "    width, height = image.size\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            # Check if the pixel is transparent\n",
        "            if pixel_data[x, y][3] == 0:\n",
        "                # Set the pixel color to black (RGB: 0, 0, 0, Alpha: 255)\n",
        "                pixel_data[x, y] = (0, 0, 0, 255)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n",
        "\n",
        "# Define your custom transformation function\n",
        "augmentation = transforms.Compose([v2.RandomPerspective(0.5),v2.RandomAffine(degrees=(-20, 20), translate=(0.1, 0.2), scale=(0.5, 0.9)), v2.ColorJitter(brightness=.1, hue=.05),v2.RandomAdjustSharpness(sharpness_factor=2)])\n",
        "\n",
        "def removeBackground(image):\n",
        "    # Apply your custom background removal function\n",
        "    processed_image = remove_background_pil(image) # remove_background(removal_model, image)\n",
        "    \n",
        "    # Apply other transformations if needed\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "        #transforms.ToTensor(),\n",
        "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    return transform(processed_image)\n",
        "\n",
        "def Augment(image):\n",
        "    # Apply your custom background removal function\n",
        "    transform = transforms.Compose([\n",
        "        augmentation,\n",
        "        transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    return transform(image)\n",
        "\n",
        "def input_transform(image):\n",
        "    # Apply your custom background removal function\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    return transform(image)\n",
        "\n",
        "normal_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n# Assuming you already have your original dataset and transformed dataset\\noriginal_dataset = datasets.ImageFolder(root=train_dataset_path, transform=normal_transform)\\n\\n# Create a DataLoader\\ndataloader = DataLoader(original_dataset, batch_size=None, shuffle=False)\\n\\ntransform_to_tensor = transforms.ToTensor()\\n\\n# Iterate through the DataLoader\\nfor image_path, target in original_dataset.imgs:\\n    class_name = original_dataset.classes[target]\\n    class_path = os.path.join(background_removed_path, class_name)\\n    os.makedirs(class_path, exist_ok=True)\\n    \\n    # Load the image\\n    image = Image.open(image_path)\\n    \\n    # Apply the transformation\\n    transformed_image = transform_to_tensor(removeBackground(image))\\n    \\n    # Save the transformed image in the corresponding class folder\\n    image_filename = os.path.basename(image_path)\\n    image_save_path = os.path.join(class_path, image_filename)\\n    transformed_image_pil = transforms.ToPILImage()(transformed_image)\\n    transformed_image_pil.save(image_save_path)  # Save the transformed image\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#COMBINE DATASET WITH RANDOMLY AUGMENTED DATASETS\n",
        "\n",
        "torch.cuda.is_available()\n",
        "True\n",
        "from torchvision.transforms import v2\n",
        "import shutil\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "background_removed_path = 'data-students/TRAIN-NOBG'\n",
        "\n",
        "'''\n",
        "\n",
        "# Assuming you already have your original dataset and transformed dataset\n",
        "original_dataset = datasets.ImageFolder(root=train_dataset_path, transform=normal_transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(original_dataset, batch_size=None, shuffle=False)\n",
        "\n",
        "transform_to_tensor = transforms.ToTensor()\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for image_path, target in original_dataset.imgs:\n",
        "    class_name = original_dataset.classes[target]\n",
        "    class_path = os.path.join(background_removed_path, class_name)\n",
        "    os.makedirs(class_path, exist_ok=True)\n",
        "    \n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "    \n",
        "    # Apply the transformation\n",
        "    transformed_image = transform_to_tensor(removeBackground(image))\n",
        "    \n",
        "    # Save the transformed image in the corresponding class folder\n",
        "    image_filename = os.path.basename(image_path)\n",
        "    image_save_path = os.path.join(class_path, image_filename)\n",
        "    transformed_image_pil = transforms.ToPILImage()(transformed_image)\n",
        "    transformed_image_pil.save(image_save_path)  # Save the transformed image\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3108\n",
            "1244\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "nobg_dataset = datasets.ImageFolder(root=background_removed_path, transform=normal_transform)\n",
        "\n",
        "# Define how many times you want to enlarge the dataset\n",
        "enlarge_factor = 5\n",
        "\n",
        "# Create a list to hold the datasets\n",
        "combined_datasets = [nobg_dataset]\n",
        "\n",
        "# Add the transformed dataset to the list multiple times\n",
        "for _ in range(enlarge_factor):\n",
        "    transformed_dataset = datasets.ImageFolder(root=background_removed_path, transform=Augment)\n",
        "    combined_datasets.append(transformed_dataset)\n",
        "\n",
        "# Concatenate the datasets into a single dataset\n",
        "enlarged_dataset = ConcatDataset(combined_datasets)\n",
        "\n",
        "\n",
        "test_set_size = 0.4\n",
        "#get train & test for K-MEANS\n",
        "\n",
        "train_val_dataset, test_dataset = train_test_split(enlarged_dataset, test_size = test_set_size, random_state=seed) #random_state = randomizer seed\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(len(enlarged_dataset))\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njIOwwsTdmsa"
      },
      "source": [
        "# Auxiliary functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIbahDOdjdcU"
      },
      "outputs": [],
      "source": [
        "#to print the label (AUXILIAR)\n",
        "label_str = [\n",
        "    \"12 - Don't Go Left or Right\",\n",
        "    \"13 - Don't Go Right\",\n",
        "    \"24 - Go Right\",\n",
        "    \"37 - Children crossing\",\n",
        "    \"38 - Dangerous curve to the right\",\n",
        "    \"39 - Dangerous curve to the left\",\n",
        "    \"44 - Go left or straight\",\n",
        "    \"50 - Fences\",\n",
        "    \"6 - Speed limit (70km/h)\"\n",
        "]\n",
        "label_str_id = [\n",
        "    \"12\",\n",
        "    \"13\",\n",
        "    \"24\",\n",
        "    \"37\",\n",
        "    \"38\",\n",
        "    \"39\",\n",
        "    \"44\",\n",
        "    \"50\",\n",
        "    \"6\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def voting(trained_models,data_loader,type, modelName):\n",
        "    #type = 1 -> data loader with no labels (test folder)\n",
        "    #type = 0 -> data loader with labels\n",
        "    \n",
        "    save_dir = \"Saved_Models\"\n",
        "    \n",
        "    # Create a directory to save the models if save_dir is provided\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    iter = len(trained_models)\n",
        "    # Save the models\n",
        "    for i, model in enumerate(trained_models):\n",
        "        filename = f\"fold_{i}_{modelName}.pth\"\n",
        "        if save_dir:\n",
        "            filename = os.path.join(save_dir, filename)\n",
        "        torch.save(model, filename)\n",
        "        \n",
        "\n",
        "    if type == 0: #data loader with labels\n",
        "        #pred matrix\n",
        "        predictions = []\n",
        "        for i in range(iter):\n",
        "            filename = f\"fold_\" + str(i) + \"_\" + modelName + \".pth\"\n",
        "            torch.save(model, filename)\n",
        "            model.eval()\n",
        "            #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "            i = 0\n",
        "            with torch.no_grad():\n",
        "                    model_predictions = []\n",
        "\n",
        "                    for inputs, labels in data_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        model_predictions.append(predicted)\n",
        "                    predictions_aux = torch.cat(model_predictions, dim=0).cpu()\n",
        "                    predictions.append(predictions_aux)\n",
        "\n",
        "        predictions_matrix = np.vstack(predictions)\n",
        "\n",
        "        #voting\n",
        "        num_classes = predictions_matrix.max() + 1\n",
        "\n",
        "        class_votes = np.zeros((num_classes, predictions_matrix.shape[1])) #(num_classes, num_predictions)\n",
        "\n",
        "        for col in range(predictions_matrix.shape[1]):\n",
        "            unique_classes, class_counts = np.unique(predictions_matrix[:, col], return_counts=True)\n",
        "            class_votes[unique_classes, col] = class_counts\n",
        "\n",
        "        most_voted_classes = np.argmax(class_votes, axis=0) #pred\n",
        "\n",
        "        #labels\n",
        "        full_dataset = []\n",
        "        for batch in data_loader:\n",
        "            inputs, labels = batch\n",
        "            full_dataset.append((inputs, labels))\n",
        "            y = torch.cat([labels for _, labels in full_dataset], dim=0) #labels\n",
        "\n",
        "        # evaluation\n",
        "        # Compute confusion matrix and F1 score\n",
        "    \n",
        "        conf_mat = confusion_matrix(y, most_voted_classes)\n",
        "        f1 = f1_score(y, most_voted_classes, average='weighted')\n",
        "        bal_acc = balanced_accuracy_score(y, most_voted_classes)\n",
        "        #precision = precision_score(y, most_voted_classes, average='weighted')\n",
        "        #recall = recall_score(y, most_voted_classes, average='weighted')\n",
        "\n",
        "        print('Confusion Matrix:\\n', conf_mat)\n",
        "        print('F1 Score: ', f1)\n",
        "        print('B_acc: ', bal_acc)\n",
        "        #print('Precision: ', precision)\n",
        "        #print('Recall: ', recall)\n",
        "\n",
        "        return y, most_voted_classes\n",
        "        \n",
        "    else: #from test folder - no labels\n",
        "        predictions = []\n",
        "        for i in range(iter):\n",
        "            model = trained_models[i]\n",
        "            model.eval()\n",
        "            #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model.to(device)\n",
        "            i = 0\n",
        "            with torch.no_grad():\n",
        "                    model_predictions = []\n",
        "\n",
        "                    for i, inputs in enumerate(data_loader):\n",
        "                        inputs = inputs.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        predicted = torch.argmax(outputs, dim=1)\n",
        "                        model_predictions.append(predicted)\n",
        "                    predictions_aux = torch.cat(model_predictions, dim=0).cpu()\n",
        "                    predictions.append(predictions_aux)\n",
        "\n",
        "        predictions_matrix = np.vstack(predictions)\n",
        "\n",
        "        #voting\n",
        "        num_classes = predictions_matrix.max() + 1\n",
        "\n",
        "        class_votes = np.zeros((num_classes, predictions_matrix.shape[1])) #(num_classes, num_predictions)\n",
        "\n",
        "        for col in range(predictions_matrix.shape[1]):\n",
        "            unique_classes, class_counts = np.unique(predictions_matrix[:, col], return_counts=True)\n",
        "            class_votes[unique_classes, col] = class_counts\n",
        "\n",
        "        most_voted_classes = np.argmax(class_votes, axis=0) #pred\n",
        "         \n",
        "        return most_voted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zhDXn_Xn-Eos"
      },
      "outputs": [],
      "source": [
        "def createCSV(model, test_dataset_loader,type, name):\n",
        "    import csv\n",
        "\n",
        "    data = []\n",
        "\n",
        "    #directory where you want to save the CSV file\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        save_dir = \"/content/drive/MyDrive/Project1-AML/Nic/\"\n",
        "    else:\n",
        "        #save_dir = r\"C:\\Users\\Nicoli Leal\\Desktop\\MEEC\\2 semestre\\Aprendizagem Computacional Avançada\\Project-1\"\n",
        "        save_dir = \"/home/stefanotrenti/AML/project/CSVs\"\n",
        "\n",
        "\n",
        "    #file name\n",
        "    csv_file = os.path.join(save_dir, name)\n",
        "\n",
        "    if type == \"voting\":\n",
        "      most_voted_classes = voting(model, test_dataset_loader,1,\"models_for_voting\")\n",
        "      for i in range(len(most_voted_classes)):\n",
        "        predicted_class = int(most_voted_classes[i])  # Extract the integer value\n",
        "        data.append({\"ID\": i+1, \"Class\": label_str_id[predicted_class]}) #, \"Name\": label_str[test_predictions]})\n",
        "\n",
        "    else:\n",
        "      for i, images in enumerate(test_dataset_loader):\n",
        "\n",
        "          images = images.to(device)\n",
        "          # Forward pass\n",
        "          outputs = model(images)\n",
        "          test_predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "          images = images.cpu().numpy()\n",
        "          predicted_classes = test_predictions.cpu().numpy()\n",
        "\n",
        "          # Iterate over the batch\n",
        "          predicted_class = int(predicted_classes[0])  # Extract the integer value\n",
        "          data.append({\"ID\": i+1, \"Class\": label_str_id[predicted_class]}) #, \"Name\": label_str[test_predictions]})\n",
        "\n",
        "\n",
        "    # Define the field names\n",
        "    fields = [\"ID\", \"Class\"]#, \"Name\"]\n",
        "\n",
        "    # Write data to CSV file\n",
        "    with open(csv_file, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fields)\n",
        "\n",
        "        # Write the header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write the data rows\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(\"CSV file created successfully.\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os,sys\n",
        "\n",
        "def saveResults(model, directory, name, modelName, loss, optim):\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    \n",
        "    # Join the directory path with the file name\n",
        "    results_path = os.path.join(directory, name)\n",
        "    \n",
        "    # Check if the file exists\n",
        "    file_exists = os.path.isfile(results_path)\n",
        "    \n",
        "    # Open the file in append mode or write mode depending on whether the file exists\n",
        "    with open(results_path, 'a' if file_exists else 'w') as f:\n",
        "        # Redirect stdout to the file\n",
        "        sys.stdout = f\n",
        "        print(\" ---------------------------------------------------------------------------------------------------------------------- \")\n",
        "        print(\" ----------------------------- Model parameters ------------------------------\")\n",
        "        print(\"model name:\" + modelName)\n",
        "        print('learning rate  = 0.001')\n",
        "        print('epochs = 50')\n",
        "        print('folds = 5')\n",
        "        print('batch size = 64')\n",
        "        print('Test set size  = ',test_set_size)\n",
        "        print(\"enlarge_factor =\", enlarge_factor)\n",
        "        print('loss = '+ loss)\n",
        "        print('optimizer = '+ optim)\n",
        "        print(\" ------------------------------- Evaluation  ---------------------------------\")\n",
        "        evaluate_network(model, test_loader)\n",
        "        print(\" ---------------------------------------------------------------------------------------------------------------------- \")\n",
        " \n",
        "    # Reset stdout to its default value (console)\n",
        "    sys.stdout = sys.__stdout__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPwOZytiwZMd"
      },
      "source": [
        "# EVALUATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zgejPcIyXVOk"
      },
      "outputs": [],
      "source": [
        "def evaluate_network(model,dataset_loader, to_device=True):\n",
        "    # X given input data\n",
        "    # y corresponding target labels\n",
        "    full_dataset = []\n",
        "    for batch in dataset_loader:\n",
        "        # Assuming each batch is a tuple (inputs, labels)\n",
        "        inputs, labels = batch\n",
        "        full_dataset.append((inputs, labels))\n",
        "\n",
        "        # Concatenate all data points into a single tensor\n",
        "        X = torch.cat([inputs for inputs, _ in full_dataset], dim=0)\n",
        "        y = torch.cat([labels for _, labels in full_dataset], dim=0)\n",
        "\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    if to_device:\n",
        "      # Assuming you're using GPU (if available)\n",
        "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        #model = model.to(device)\n",
        "\n",
        "    # Run the model on the test data\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Convert tensors to numpy arrays\n",
        "    if to_device:\n",
        "        predicted = predicted.to(\"cpu\")\n",
        "\n",
        "    predicted_np = predicted.cpu().numpy()\n",
        "    test_target_np = y.cpu().numpy()\n",
        "\n",
        "\n",
        "    # Compute confusion matrix and F1 score\n",
        "    conf_mat = confusion_matrix(test_target_np, predicted_np)\n",
        "    f1 = f1_score(test_target_np, predicted_np, average='weighted')\n",
        "    bal_acc = balanced_accuracy_score(y.cpu(), predicted_np)\n",
        "\n",
        "    #print('Confusion Matrix:\\n', conf_mat)\n",
        "    print('F1 Score: ', f1)\n",
        "    print('B_acc: ', bal_acc)\n",
        "\n",
        "    return conf_mat, f1, bal_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAIN Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainCNN(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model, val_acc, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Train_K_FOLDS(k_model, epochs, lr, folds, batch_size, loss_type, optim_type):\n",
        "    \n",
        "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "    \n",
        "    print(\"Optimizer type:\", optim_type)\n",
        "    print(\"Loss type:\", loss_type)\n",
        "\n",
        "    models = []\n",
        "    ordered_models = []\n",
        "    index=0\n",
        "    i = 0\n",
        "    best_accuracy = 0\n",
        "    val_values = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_val_dataset)):\n",
        "        \n",
        "        i= i+1\n",
        "        \n",
        "        model = k_model()\n",
        "\n",
        "        #Creating DataLoaders for training and validation\n",
        "        train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "        val_sampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
        "        #\n",
        "        train_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "        #CHOOSE LOSS\n",
        "        if loss_type == 'MSELoss' or loss_type == 'BCELoss':\n",
        "            criterion = nn.MSELoss()\n",
        "            \n",
        "        elif loss_type == 'cross_entropy':\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "        elif loss_type == 'BCELoss':\n",
        "            criterion = nn.BCELoss()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
        "            \n",
        "        #CHOOSE OPTIMIZER\n",
        "        if optim_type == 'adam':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        elif optim_type == 'SGD':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "        elif optim_type == 'RMSprop':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported optimizer type: {optim_type}\")\n",
        "            \n",
        "        # Train the model\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        if loss_type == 'MSELoss' or loss_type == 'BCELoss':\n",
        "            trained_model, val_acc, val_loss = trainCNN_mse(model, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
        "        else:\n",
        "            trained_model, val_acc, val_loss = trainCNN(model, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
        "        \n",
        "        models.append(trained_model)\n",
        "        #conf_matrix, F_score, bal_acc = evaluate_network(trained_model, val_loader)\n",
        "        \n",
        "        val_values.append(val_acc)\n",
        "        \n",
        "        if(val_acc > best_accuracy):\n",
        "            best_accuracy = val_acc\n",
        "            index = i\n",
        "            best_model = model\n",
        "        \n",
        "\n",
        "        print(\"Best model was with fold number \", index , \"\\n\")\n",
        "          \n",
        "    # Create a list of tuples with (value, index)\n",
        "    indexed_values = [(value, index) for index, value in enumerate(val_values)]\n",
        "    # Sort the list of tuples based on the values (in descending order)\n",
        "    sorted_values = sorted(indexed_values, key=lambda x: x[0], reverse=True)\n",
        "    # Extract the sorted indexes\n",
        "    sorted_indexes = [index for value, index in sorted_values]\n",
        "    \n",
        "    first_value = models[sorted_indexes[0]]\n",
        "    second_value = models[sorted_indexes[1]]\n",
        "    third = models[sorted_indexes[2]]\n",
        "    fourth = models[sorted_indexes[3]]\n",
        "    fifth = models[sorted_indexes[4]]\n",
        "    ordered_models.append(first_value)  \n",
        "    ordered_models.append(second_value)\n",
        "    ordered_models.append(third)\n",
        "    ordered_models.append(fourth)\n",
        "    ordered_models.append(fifth)    \n",
        "        \n",
        "    return best_model, ordered_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef Train_K_FOLDS(k_model, epochs, lr, folds, batch_size):\\n    \\n    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\\n\\n    models = []\\n    ordered_models = []\\n    index=0\\n    i = 0\\n    best_accuracy = 0\\n    val_values = []\\n\\n    for fold, (train_index, val_index) in enumerate(kf.split(train_val_dataset)):\\n        \\n        i= i+1\\n        \\n        model = k_model()\\n\\n        #Creating DataLoaders for training and validation\\n        train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\\n        val_sampler = torch.utils.data.SubsetRandomSampler(val_index)\\n        #\\n        train_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=train_sampler)\\n        val_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=val_sampler)\\n\\n        # Define your loss function and optimizer\\n        criterion = nn.CrossEntropyLoss()\\n        optimizer = optim.Adam(model.parameters(), lr=lr)\\n\\n        # Train the model\\n\\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n        trained_model, val_acc, val_loss = trainCNN(model, train_loader, val_loader, criterion, optimizer, epochs, device)\\n        models.append(trained_model)\\n        #conf_matrix, F_score, bal_acc = evaluate_network(trained_model, val_loader)\\n        \\n        val_values.append(val_acc)\\n        \\n        if(val_acc > best_accuracy):\\n            best_accuracy = val_acc\\n            index = i\\n            best_model = model\\n        \\n\\n        print(\"Best model was with fold number \", index , \"\\n\")\\n          \\n    # Create a list of tuples with (value, index)\\n    indexed_values = [(value, index) for index, value in enumerate(val_values)]\\n    # Sort the list of tuples based on the values (in descending order)\\n    sorted_values = sorted(indexed_values, key=lambda x: x[0], reverse=True)\\n    # Extract the sorted indexes\\n    sorted_indexes = [index for value, index in sorted_values]\\n    \\n    first_value = models[sorted_indexes[0]]\\n    second_value = models[sorted_indexes[1]]\\n    third = models[sorted_indexes[2]]\\n    fourth = models[sorted_indexes[3]]\\n    fifth = models[sorted_indexes[4]]\\n    ordered_models.append(first_value)  \\n    ordered_models.append(second_value)\\n    ordered_models.append(third)\\n    ordered_models.append(fourth)\\n    ordered_models.append(fifth)    \\n        \\n    return best_model, ordered_models\\n    '"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "def Train_K_FOLDS(k_model, epochs, lr, folds, batch_size):\n",
        "    \n",
        "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "\n",
        "    models = []\n",
        "    ordered_models = []\n",
        "    index=0\n",
        "    i = 0\n",
        "    best_accuracy = 0\n",
        "    val_values = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_val_dataset)):\n",
        "        \n",
        "        i= i+1\n",
        "        \n",
        "        model = k_model()\n",
        "\n",
        "        #Creating DataLoaders for training and validation\n",
        "        train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "        val_sampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
        "        #\n",
        "        train_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader = DataLoader(train_val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "        # Define your loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # Train the model\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        trained_model, val_acc, val_loss = trainCNN(model, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
        "        models.append(trained_model)\n",
        "        #conf_matrix, F_score, bal_acc = evaluate_network(trained_model, val_loader)\n",
        "        \n",
        "        val_values.append(val_acc)\n",
        "        \n",
        "        if(val_acc > best_accuracy):\n",
        "            best_accuracy = val_acc\n",
        "            index = i\n",
        "            best_model = model\n",
        "        \n",
        "\n",
        "        print(\"Best model was with fold number \", index , \"\\n\")\n",
        "          \n",
        "    # Create a list of tuples with (value, index)\n",
        "    indexed_values = [(value, index) for index, value in enumerate(val_values)]\n",
        "    # Sort the list of tuples based on the values (in descending order)\n",
        "    sorted_values = sorted(indexed_values, key=lambda x: x[0], reverse=True)\n",
        "    # Extract the sorted indexes\n",
        "    sorted_indexes = [index for value, index in sorted_values]\n",
        "    \n",
        "    first_value = models[sorted_indexes[0]]\n",
        "    second_value = models[sorted_indexes[1]]\n",
        "    third = models[sorted_indexes[2]]\n",
        "    fourth = models[sorted_indexes[3]]\n",
        "    fifth = models[sorted_indexes[4]]\n",
        "    ordered_models.append(first_value)  \n",
        "    ordered_models.append(second_value)\n",
        "    ordered_models.append(third)\n",
        "    ordered_models.append(fourth)\n",
        "    ordered_models.append(fifth)    \n",
        "        \n",
        "    return best_model, ordered_models\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24b2DZWRXS1"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DV_CNN_v3(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=9):\n",
        "        super(DV_CNN_v3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        self.conv5 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.conv6 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.fc1 = nn.Linear(9 * 9 * 16, 120)\n",
        "        self.fc2 = nn.Linear(120, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(F.relu(self.bn5(self.conv5(x))), 2)\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "\n",
        "        x = x.view(-1, self.fc1.in_features)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def print_architecture(self):\n",
        "        print(self)\n",
        "        \n",
        "    def get_architecture_string(self):\n",
        "        return str(self)\n",
        "    \n",
        "    def get_architecture_name(self):\n",
        "        return str('DV_CNN_v3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAINING THE CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer type: SGD\n",
            "Loss type: cross_entropy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stefanotrenti/miniconda3/envs/AML-env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Train Loss: 2.1887, Train Acc: 15.69%, Val Loss: 2.1884, Val Acc: 14.75%\n",
            "Epoch [2/50], Train Loss: 2.1312, Train Acc: 26.09%, Val Loss: 2.1196, Val Acc: 26.27%\n",
            "Epoch [3/50], Train Loss: 2.0894, Train Acc: 29.04%, Val Loss: 2.0732, Val Acc: 27.35%\n",
            "Epoch [4/50], Train Loss: 2.0555, Train Acc: 29.51%, Val Loss: 2.0354, Val Acc: 29.22%\n",
            "Epoch [5/50], Train Loss: 2.0224, Train Acc: 31.19%, Val Loss: 2.0040, Val Acc: 29.22%\n",
            "Epoch [6/50], Train Loss: 1.9892, Train Acc: 31.19%, Val Loss: 1.9705, Val Acc: 29.22%\n",
            "Epoch [7/50], Train Loss: 1.9574, Train Acc: 31.99%, Val Loss: 1.9434, Val Acc: 30.29%\n",
            "Epoch [8/50], Train Loss: 1.9234, Train Acc: 32.86%, Val Loss: 1.9079, Val Acc: 29.76%\n",
            "Epoch [9/50], Train Loss: 1.8830, Train Acc: 34.27%, Val Loss: 1.8679, Val Acc: 32.17%\n",
            "Epoch [10/50], Train Loss: 1.8599, Train Acc: 34.88%, Val Loss: 1.8308, Val Acc: 33.24%\n",
            "Epoch [11/50], Train Loss: 1.8044, Train Acc: 37.69%, Val Loss: 1.7935, Val Acc: 34.58%\n",
            "Epoch [12/50], Train Loss: 1.7788, Train Acc: 37.56%, Val Loss: 1.7358, Val Acc: 36.19%\n",
            "Epoch [13/50], Train Loss: 1.7214, Train Acc: 39.50%, Val Loss: 1.6861, Val Acc: 38.34%\n",
            "Epoch [14/50], Train Loss: 1.6770, Train Acc: 42.66%, Val Loss: 1.6505, Val Acc: 39.68%\n",
            "Epoch [15/50], Train Loss: 1.6517, Train Acc: 43.86%, Val Loss: 1.6090, Val Acc: 44.50%\n",
            "Epoch [16/50], Train Loss: 1.6042, Train Acc: 47.42%, Val Loss: 1.5685, Val Acc: 48.79%\n",
            "Epoch [17/50], Train Loss: 1.5679, Train Acc: 48.56%, Val Loss: 1.5408, Val Acc: 51.47%\n",
            "Epoch [18/50], Train Loss: 1.5382, Train Acc: 51.64%, Val Loss: 1.4992, Val Acc: 53.62%\n",
            "Epoch [19/50], Train Loss: 1.5034, Train Acc: 52.38%, Val Loss: 1.4824, Val Acc: 55.50%\n",
            "Epoch [20/50], Train Loss: 1.4709, Train Acc: 54.73%, Val Loss: 1.4309, Val Acc: 57.37%\n",
            "Epoch [21/50], Train Loss: 1.4348, Train Acc: 55.13%, Val Loss: 1.4051, Val Acc: 58.45%\n",
            "Epoch [22/50], Train Loss: 1.4175, Train Acc: 56.07%, Val Loss: 1.3737, Val Acc: 60.05%\n",
            "Epoch [23/50], Train Loss: 1.4090, Train Acc: 57.08%, Val Loss: 1.3489, Val Acc: 59.79%\n",
            "Epoch [24/50], Train Loss: 1.3665, Train Acc: 58.48%, Val Loss: 1.3300, Val Acc: 61.13%\n",
            "Epoch [25/50], Train Loss: 1.3334, Train Acc: 58.69%, Val Loss: 1.3045, Val Acc: 61.13%\n",
            "Epoch [26/50], Train Loss: 1.3099, Train Acc: 59.96%, Val Loss: 1.2881, Val Acc: 63.00%\n",
            "Epoch [27/50], Train Loss: 1.2849, Train Acc: 59.96%, Val Loss: 1.2610, Val Acc: 60.32%\n",
            "Epoch [28/50], Train Loss: 1.2636, Train Acc: 60.63%, Val Loss: 1.2404, Val Acc: 63.00%\n",
            "Epoch [29/50], Train Loss: 1.2503, Train Acc: 60.43%, Val Loss: 1.2098, Val Acc: 63.27%\n",
            "Epoch [30/50], Train Loss: 1.2263, Train Acc: 61.23%, Val Loss: 1.1957, Val Acc: 62.73%\n",
            "Epoch [31/50], Train Loss: 1.2166, Train Acc: 61.57%, Val Loss: 1.1804, Val Acc: 62.20%\n",
            "Epoch [32/50], Train Loss: 1.1876, Train Acc: 63.11%, Val Loss: 1.1473, Val Acc: 63.81%\n",
            "Epoch [33/50], Train Loss: 1.1776, Train Acc: 62.98%, Val Loss: 1.1528, Val Acc: 64.88%\n",
            "Epoch [34/50], Train Loss: 1.1512, Train Acc: 64.45%, Val Loss: 1.1164, Val Acc: 65.68%\n",
            "Epoch [35/50], Train Loss: 1.1544, Train Acc: 64.32%, Val Loss: 1.1442, Val Acc: 65.95%\n",
            "Epoch [36/50], Train Loss: 1.1297, Train Acc: 65.19%, Val Loss: 1.1137, Val Acc: 64.34%\n",
            "Epoch [37/50], Train Loss: 1.1145, Train Acc: 65.59%, Val Loss: 1.1126, Val Acc: 62.73%\n",
            "Epoch [38/50], Train Loss: 1.1052, Train Acc: 65.86%, Val Loss: 1.0635, Val Acc: 67.02%\n",
            "Epoch [39/50], Train Loss: 1.0834, Train Acc: 66.67%, Val Loss: 1.0531, Val Acc: 67.83%\n",
            "Epoch [40/50], Train Loss: 1.0679, Train Acc: 66.67%, Val Loss: 1.0490, Val Acc: 68.10%\n",
            "Epoch [41/50], Train Loss: 1.0390, Train Acc: 67.20%, Val Loss: 1.0223, Val Acc: 68.63%\n",
            "Epoch [42/50], Train Loss: 1.0252, Train Acc: 67.47%, Val Loss: 1.0239, Val Acc: 68.10%\n",
            "Epoch [43/50], Train Loss: 1.0120, Train Acc: 68.48%, Val Loss: 0.9956, Val Acc: 69.17%\n",
            "Epoch [44/50], Train Loss: 0.9964, Train Acc: 68.48%, Val Loss: 0.9848, Val Acc: 69.17%\n",
            "Epoch [45/50], Train Loss: 0.9875, Train Acc: 68.14%, Val Loss: 0.9702, Val Acc: 69.71%\n",
            "Epoch [46/50], Train Loss: 0.9775, Train Acc: 67.94%, Val Loss: 1.0093, Val Acc: 66.22%\n",
            "Epoch [47/50], Train Loss: 0.9566, Train Acc: 68.88%, Val Loss: 1.0562, Val Acc: 65.95%\n",
            "Epoch [48/50], Train Loss: 0.9508, Train Acc: 69.08%, Val Loss: 1.0181, Val Acc: 67.56%\n",
            "Epoch [49/50], Train Loss: 0.9303, Train Acc: 69.89%, Val Loss: 0.9703, Val Acc: 68.63%\n",
            "Epoch [50/50], Train Loss: 0.9069, Train Acc: 69.82%, Val Loss: 0.9624, Val Acc: 68.63%\n",
            "Finished Training\n",
            "Best model was with fold number  1 \n",
            "\n",
            "Epoch [1/50], Train Loss: 2.1448, Train Acc: 17.04%, Val Loss: 2.1726, Val Acc: 19.30%\n",
            "Epoch [2/50], Train Loss: 2.1012, Train Acc: 18.65%, Val Loss: 2.0965, Val Acc: 19.84%\n",
            "Epoch [3/50], Train Loss: 2.0639, Train Acc: 19.38%, Val Loss: 2.0500, Val Acc: 21.45%\n",
            "Epoch [4/50], Train Loss: 2.0289, Train Acc: 25.29%, Val Loss: 2.0197, Val Acc: 24.93%\n",
            "Epoch [5/50], Train Loss: 1.9979, Train Acc: 29.38%, Val Loss: 1.9994, Val Acc: 28.42%\n",
            "Epoch [6/50], Train Loss: 1.9755, Train Acc: 31.59%, Val Loss: 1.9744, Val Acc: 31.37%\n",
            "Epoch [7/50], Train Loss: 1.9560, Train Acc: 32.66%, Val Loss: 1.9594, Val Acc: 32.71%\n",
            "Epoch [8/50], Train Loss: 1.9338, Train Acc: 33.47%, Val Loss: 1.9408, Val Acc: 33.24%\n",
            "Epoch [9/50], Train Loss: 1.9087, Train Acc: 35.61%, Val Loss: 1.9171, Val Acc: 35.39%\n",
            "Epoch [10/50], Train Loss: 1.8934, Train Acc: 37.56%, Val Loss: 1.9007, Val Acc: 37.27%\n",
            "Epoch [11/50], Train Loss: 1.8708, Train Acc: 38.83%, Val Loss: 1.8791, Val Acc: 37.80%\n",
            "Epoch [12/50], Train Loss: 1.8565, Train Acc: 39.91%, Val Loss: 1.8688, Val Acc: 39.41%\n",
            "Epoch [13/50], Train Loss: 1.8323, Train Acc: 41.45%, Val Loss: 1.8548, Val Acc: 39.95%\n",
            "Epoch [14/50], Train Loss: 1.8120, Train Acc: 41.38%, Val Loss: 1.8327, Val Acc: 40.75%\n",
            "Epoch [15/50], Train Loss: 1.7921, Train Acc: 41.65%, Val Loss: 1.8182, Val Acc: 40.75%\n",
            "Epoch [16/50], Train Loss: 1.7651, Train Acc: 43.93%, Val Loss: 1.7979, Val Acc: 41.29%\n",
            "Epoch [17/50], Train Loss: 1.7493, Train Acc: 43.93%, Val Loss: 1.7803, Val Acc: 41.29%\n",
            "Epoch [18/50], Train Loss: 1.7273, Train Acc: 44.80%, Val Loss: 1.7540, Val Acc: 41.55%\n",
            "Epoch [19/50], Train Loss: 1.7001, Train Acc: 45.00%, Val Loss: 1.7297, Val Acc: 42.90%\n",
            "Epoch [20/50], Train Loss: 1.6748, Train Acc: 45.94%, Val Loss: 1.7081, Val Acc: 44.24%\n",
            "Epoch [21/50], Train Loss: 1.6457, Train Acc: 47.75%, Val Loss: 1.6787, Val Acc: 42.90%\n",
            "Epoch [22/50], Train Loss: 1.6215, Train Acc: 49.09%, Val Loss: 1.6592, Val Acc: 43.70%\n",
            "Epoch [23/50], Train Loss: 1.5930, Train Acc: 49.70%, Val Loss: 1.6271, Val Acc: 45.04%\n",
            "Epoch [24/50], Train Loss: 1.5508, Train Acc: 51.24%, Val Loss: 1.5933, Val Acc: 44.77%\n",
            "Epoch [25/50], Train Loss: 1.5216, Train Acc: 52.58%, Val Loss: 1.5674, Val Acc: 45.04%\n",
            "Epoch [26/50], Train Loss: 1.4948, Train Acc: 52.58%, Val Loss: 1.5298, Val Acc: 46.65%\n",
            "Epoch [27/50], Train Loss: 1.4739, Train Acc: 53.25%, Val Loss: 1.4853, Val Acc: 48.53%\n",
            "Epoch [28/50], Train Loss: 1.4271, Train Acc: 54.39%, Val Loss: 1.4510, Val Acc: 50.40%\n",
            "Epoch [29/50], Train Loss: 1.3868, Train Acc: 55.40%, Val Loss: 1.4422, Val Acc: 49.06%\n",
            "Epoch [30/50], Train Loss: 1.3620, Train Acc: 57.01%, Val Loss: 1.3890, Val Acc: 53.08%\n",
            "Epoch [31/50], Train Loss: 1.3321, Train Acc: 57.01%, Val Loss: 1.3828, Val Acc: 54.42%\n",
            "Epoch [32/50], Train Loss: 1.3014, Train Acc: 57.28%, Val Loss: 1.3359, Val Acc: 53.35%\n",
            "Epoch [33/50], Train Loss: 1.2757, Train Acc: 58.69%, Val Loss: 1.3281, Val Acc: 54.96%\n",
            "Epoch [34/50], Train Loss: 1.2525, Train Acc: 60.50%, Val Loss: 1.2832, Val Acc: 55.76%\n",
            "Epoch [35/50], Train Loss: 1.2227, Train Acc: 60.63%, Val Loss: 1.2690, Val Acc: 56.30%\n",
            "Epoch [36/50], Train Loss: 1.2089, Train Acc: 60.83%, Val Loss: 1.2562, Val Acc: 55.23%\n",
            "Epoch [37/50], Train Loss: 1.1782, Train Acc: 62.37%, Val Loss: 1.2305, Val Acc: 57.10%\n",
            "Epoch [38/50], Train Loss: 1.1562, Train Acc: 62.11%, Val Loss: 1.2052, Val Acc: 57.64%\n",
            "Epoch [39/50], Train Loss: 1.1463, Train Acc: 62.84%, Val Loss: 1.1960, Val Acc: 57.10%\n",
            "Epoch [40/50], Train Loss: 1.1235, Train Acc: 63.78%, Val Loss: 1.1638, Val Acc: 60.05%\n",
            "Epoch [41/50], Train Loss: 1.0978, Train Acc: 64.99%, Val Loss: 1.1527, Val Acc: 60.05%\n",
            "Epoch [42/50], Train Loss: 1.0864, Train Acc: 64.79%, Val Loss: 1.1338, Val Acc: 61.39%\n",
            "Epoch [43/50], Train Loss: 1.0738, Train Acc: 66.06%, Val Loss: 1.1263, Val Acc: 63.00%\n",
            "Epoch [44/50], Train Loss: 1.0570, Train Acc: 65.46%, Val Loss: 1.1000, Val Acc: 63.54%\n",
            "Epoch [45/50], Train Loss: 1.0399, Train Acc: 67.20%, Val Loss: 1.0849, Val Acc: 62.73%\n",
            "Epoch [46/50], Train Loss: 1.0292, Train Acc: 67.00%, Val Loss: 1.0796, Val Acc: 64.34%\n",
            "Epoch [47/50], Train Loss: 1.0189, Train Acc: 67.67%, Val Loss: 1.0957, Val Acc: 61.93%\n",
            "Epoch [48/50], Train Loss: 0.9861, Train Acc: 69.08%, Val Loss: 1.0346, Val Acc: 66.49%\n",
            "Epoch [49/50], Train Loss: 0.9762, Train Acc: 68.95%, Val Loss: 1.0096, Val Acc: 66.22%\n",
            "Epoch [50/50], Train Loss: 0.9568, Train Acc: 69.89%, Val Loss: 1.0142, Val Acc: 66.76%\n",
            "Finished Training\n",
            "Best model was with fold number  1 \n",
            "\n",
            "Epoch [1/50], Train Loss: 2.1581, Train Acc: 20.05%, Val Loss: 2.1812, Val Acc: 19.57%\n",
            "Epoch [2/50], Train Loss: 2.1180, Train Acc: 22.94%, Val Loss: 2.1018, Val Acc: 20.91%\n",
            "Epoch [3/50], Train Loss: 2.0698, Train Acc: 25.96%, Val Loss: 2.0496, Val Acc: 22.25%\n",
            "Epoch [4/50], Train Loss: 2.0211, Train Acc: 26.49%, Val Loss: 2.0074, Val Acc: 24.66%\n",
            "Epoch [5/50], Train Loss: 1.9735, Train Acc: 27.57%, Val Loss: 1.9585, Val Acc: 25.74%\n",
            "Epoch [6/50], Train Loss: 1.9210, Train Acc: 29.18%, Val Loss: 1.9199, Val Acc: 26.54%\n",
            "Epoch [7/50], Train Loss: 1.8735, Train Acc: 30.85%, Val Loss: 1.8747, Val Acc: 28.95%\n",
            "Epoch [8/50], Train Loss: 1.8261, Train Acc: 32.93%, Val Loss: 1.8359, Val Acc: 30.56%\n",
            "Epoch [9/50], Train Loss: 1.7889, Train Acc: 34.00%, Val Loss: 1.8010, Val Acc: 33.24%\n",
            "Epoch [10/50], Train Loss: 1.7491, Train Acc: 35.68%, Val Loss: 1.7588, Val Acc: 32.71%\n",
            "Epoch [11/50], Train Loss: 1.7153, Train Acc: 37.02%, Val Loss: 1.7196, Val Acc: 35.66%\n",
            "Epoch [12/50], Train Loss: 1.6821, Train Acc: 38.56%, Val Loss: 1.6970, Val Acc: 37.53%\n",
            "Epoch [13/50], Train Loss: 1.6578, Train Acc: 41.05%, Val Loss: 1.6646, Val Acc: 39.95%\n",
            "Epoch [14/50], Train Loss: 1.6258, Train Acc: 43.06%, Val Loss: 1.6387, Val Acc: 42.90%\n",
            "Epoch [15/50], Train Loss: 1.5893, Train Acc: 46.14%, Val Loss: 1.6026, Val Acc: 44.50%\n",
            "Epoch [16/50], Train Loss: 1.5741, Train Acc: 48.02%, Val Loss: 1.5794, Val Acc: 47.72%\n",
            "Epoch [17/50], Train Loss: 1.5446, Train Acc: 49.16%, Val Loss: 1.5518, Val Acc: 50.67%\n",
            "Epoch [18/50], Train Loss: 1.5111, Train Acc: 50.70%, Val Loss: 1.5324, Val Acc: 53.35%\n",
            "Epoch [19/50], Train Loss: 1.4956, Train Acc: 51.91%, Val Loss: 1.4903, Val Acc: 54.42%\n",
            "Epoch [20/50], Train Loss: 1.4793, Train Acc: 53.52%, Val Loss: 1.4647, Val Acc: 54.69%\n",
            "Epoch [21/50], Train Loss: 1.4464, Train Acc: 54.66%, Val Loss: 1.4525, Val Acc: 56.03%\n",
            "Epoch [22/50], Train Loss: 1.4286, Train Acc: 55.94%, Val Loss: 1.4214, Val Acc: 56.84%\n",
            "Epoch [23/50], Train Loss: 1.3995, Train Acc: 55.60%, Val Loss: 1.4126, Val Acc: 57.64%\n",
            "Epoch [24/50], Train Loss: 1.3795, Train Acc: 56.81%, Val Loss: 1.3698, Val Acc: 58.45%\n",
            "Epoch [25/50], Train Loss: 1.3627, Train Acc: 57.41%, Val Loss: 1.3488, Val Acc: 58.98%\n",
            "Epoch [26/50], Train Loss: 1.3420, Train Acc: 58.08%, Val Loss: 1.3293, Val Acc: 59.52%\n",
            "Epoch [27/50], Train Loss: 1.3171, Train Acc: 59.02%, Val Loss: 1.3041, Val Acc: 60.32%\n",
            "Epoch [28/50], Train Loss: 1.3137, Train Acc: 59.69%, Val Loss: 1.2866, Val Acc: 62.47%\n",
            "Epoch [29/50], Train Loss: 1.2847, Train Acc: 59.76%, Val Loss: 1.2728, Val Acc: 60.86%\n",
            "Epoch [30/50], Train Loss: 1.2526, Train Acc: 60.70%, Val Loss: 1.2535, Val Acc: 63.27%\n",
            "Epoch [31/50], Train Loss: 1.2384, Train Acc: 60.70%, Val Loss: 1.2345, Val Acc: 63.54%\n",
            "Epoch [32/50], Train Loss: 1.2274, Train Acc: 61.10%, Val Loss: 1.2031, Val Acc: 63.00%\n",
            "Epoch [33/50], Train Loss: 1.2317, Train Acc: 62.91%, Val Loss: 1.1982, Val Acc: 62.47%\n",
            "Epoch [34/50], Train Loss: 1.1910, Train Acc: 62.17%, Val Loss: 1.1674, Val Acc: 63.54%\n",
            "Epoch [35/50], Train Loss: 1.1729, Train Acc: 63.31%, Val Loss: 1.1650, Val Acc: 63.00%\n",
            "Epoch [36/50], Train Loss: 1.1512, Train Acc: 63.11%, Val Loss: 1.1834, Val Acc: 61.66%\n",
            "Epoch [37/50], Train Loss: 1.1377, Train Acc: 64.05%, Val Loss: 1.1319, Val Acc: 60.86%\n",
            "Epoch [38/50], Train Loss: 1.1275, Train Acc: 63.51%, Val Loss: 1.1081, Val Acc: 66.22%\n",
            "Epoch [39/50], Train Loss: 1.1085, Train Acc: 64.72%, Val Loss: 1.0807, Val Acc: 65.68%\n",
            "Epoch [40/50], Train Loss: 1.0907, Train Acc: 64.65%, Val Loss: 1.0655, Val Acc: 66.49%\n",
            "Epoch [41/50], Train Loss: 1.0863, Train Acc: 65.06%, Val Loss: 1.0517, Val Acc: 67.29%\n",
            "Epoch [42/50], Train Loss: 1.0798, Train Acc: 65.59%, Val Loss: 1.0493, Val Acc: 67.83%\n",
            "Epoch [43/50], Train Loss: 1.0551, Train Acc: 65.33%, Val Loss: 1.0284, Val Acc: 66.76%\n",
            "Epoch [44/50], Train Loss: 1.0488, Train Acc: 66.26%, Val Loss: 1.0425, Val Acc: 68.36%\n",
            "Epoch [45/50], Train Loss: 1.0365, Train Acc: 66.67%, Val Loss: 1.0443, Val Acc: 67.02%\n",
            "Epoch [46/50], Train Loss: 1.0224, Train Acc: 66.73%, Val Loss: 1.0342, Val Acc: 69.97%\n",
            "Epoch [47/50], Train Loss: 0.9970, Train Acc: 67.07%, Val Loss: 0.9804, Val Acc: 69.44%\n",
            "Epoch [48/50], Train Loss: 0.9947, Train Acc: 67.74%, Val Loss: 0.9744, Val Acc: 68.90%\n",
            "Epoch [49/50], Train Loss: 0.9761, Train Acc: 68.68%, Val Loss: 0.9531, Val Acc: 71.58%\n",
            "Epoch [50/50], Train Loss: 0.9559, Train Acc: 68.34%, Val Loss: 0.9422, Val Acc: 71.58%\n",
            "Finished Training\n",
            "Best model was with fold number  3 \n",
            "\n",
            "Epoch [1/50], Train Loss: 2.1437, Train Acc: 17.71%, Val Loss: 2.1738, Val Acc: 16.09%\n",
            "Epoch [2/50], Train Loss: 2.0887, Train Acc: 20.46%, Val Loss: 2.1031, Val Acc: 14.21%\n",
            "Epoch [3/50], Train Loss: 2.0501, Train Acc: 20.79%, Val Loss: 2.0693, Val Acc: 16.35%\n",
            "Epoch [4/50], Train Loss: 2.0141, Train Acc: 23.61%, Val Loss: 2.0377, Val Acc: 20.38%\n",
            "Epoch [5/50], Train Loss: 1.9769, Train Acc: 27.16%, Val Loss: 2.0059, Val Acc: 29.49%\n",
            "Epoch [6/50], Train Loss: 1.9379, Train Acc: 32.80%, Val Loss: 1.9603, Val Acc: 32.98%\n",
            "Epoch [7/50], Train Loss: 1.8935, Train Acc: 37.09%, Val Loss: 1.9163, Val Acc: 35.92%\n",
            "Epoch [8/50], Train Loss: 1.8453, Train Acc: 39.24%, Val Loss: 1.8711, Val Acc: 36.19%\n",
            "Epoch [9/50], Train Loss: 1.7937, Train Acc: 40.64%, Val Loss: 1.8344, Val Acc: 36.73%\n",
            "Epoch [10/50], Train Loss: 1.7557, Train Acc: 41.18%, Val Loss: 1.7929, Val Acc: 37.00%\n",
            "Epoch [11/50], Train Loss: 1.7131, Train Acc: 41.72%, Val Loss: 1.7733, Val Acc: 39.41%\n",
            "Epoch [12/50], Train Loss: 1.6745, Train Acc: 43.53%, Val Loss: 1.7385, Val Acc: 40.75%\n",
            "Epoch [13/50], Train Loss: 1.6465, Train Acc: 44.67%, Val Loss: 1.7079, Val Acc: 41.55%\n",
            "Epoch [14/50], Train Loss: 1.6220, Train Acc: 45.94%, Val Loss: 1.6711, Val Acc: 42.36%\n",
            "Epoch [15/50], Train Loss: 1.5939, Train Acc: 47.35%, Val Loss: 1.6471, Val Acc: 42.90%\n",
            "Epoch [16/50], Train Loss: 1.5610, Train Acc: 48.56%, Val Loss: 1.6218, Val Acc: 43.70%\n",
            "Epoch [17/50], Train Loss: 1.5371, Train Acc: 49.77%, Val Loss: 1.6057, Val Acc: 45.04%\n",
            "Epoch [18/50], Train Loss: 1.5162, Train Acc: 50.17%, Val Loss: 1.5841, Val Acc: 46.38%\n",
            "Epoch [19/50], Train Loss: 1.4862, Train Acc: 50.97%, Val Loss: 1.5635, Val Acc: 46.38%\n",
            "Epoch [20/50], Train Loss: 1.4723, Train Acc: 53.19%, Val Loss: 1.5449, Val Acc: 48.26%\n",
            "Epoch [21/50], Train Loss: 1.4590, Train Acc: 53.66%, Val Loss: 1.5324, Val Acc: 46.38%\n",
            "Epoch [22/50], Train Loss: 1.4266, Train Acc: 55.87%, Val Loss: 1.5139, Val Acc: 46.38%\n",
            "Epoch [23/50], Train Loss: 1.4184, Train Acc: 56.34%, Val Loss: 1.4596, Val Acc: 49.33%\n",
            "Epoch [24/50], Train Loss: 1.3828, Train Acc: 57.28%, Val Loss: 1.4494, Val Acc: 50.40%\n",
            "Epoch [25/50], Train Loss: 1.3602, Train Acc: 59.02%, Val Loss: 1.4373, Val Acc: 51.74%\n",
            "Epoch [26/50], Train Loss: 1.3510, Train Acc: 58.82%, Val Loss: 1.4135, Val Acc: 52.55%\n",
            "Epoch [27/50], Train Loss: 1.3222, Train Acc: 59.29%, Val Loss: 1.4097, Val Acc: 52.55%\n",
            "Epoch [28/50], Train Loss: 1.3107, Train Acc: 59.96%, Val Loss: 1.5514, Val Acc: 49.33%\n",
            "Epoch [29/50], Train Loss: 1.2873, Train Acc: 60.56%, Val Loss: 1.3449, Val Acc: 53.62%\n",
            "Epoch [30/50], Train Loss: 1.2831, Train Acc: 60.83%, Val Loss: 1.3409, Val Acc: 56.30%\n",
            "Epoch [31/50], Train Loss: 1.2506, Train Acc: 60.76%, Val Loss: 1.3273, Val Acc: 53.08%\n",
            "Epoch [32/50], Train Loss: 1.2314, Train Acc: 61.90%, Val Loss: 1.3099, Val Acc: 58.45%\n",
            "Epoch [33/50], Train Loss: 1.2220, Train Acc: 62.31%, Val Loss: 1.2849, Val Acc: 59.25%\n",
            "Epoch [34/50], Train Loss: 1.1983, Train Acc: 62.71%, Val Loss: 1.3079, Val Acc: 56.57%\n",
            "Epoch [35/50], Train Loss: 1.1797, Train Acc: 63.18%, Val Loss: 1.2497, Val Acc: 59.25%\n",
            "Epoch [36/50], Train Loss: 1.1602, Train Acc: 63.65%, Val Loss: 1.2548, Val Acc: 57.64%\n",
            "Epoch [37/50], Train Loss: 1.1613, Train Acc: 63.38%, Val Loss: 1.2933, Val Acc: 56.57%\n",
            "Epoch [38/50], Train Loss: 1.1410, Train Acc: 63.72%, Val Loss: 1.2470, Val Acc: 59.52%\n",
            "Epoch [39/50], Train Loss: 1.1300, Train Acc: 63.65%, Val Loss: 1.1856, Val Acc: 59.25%\n",
            "Epoch [40/50], Train Loss: 1.1067, Train Acc: 64.45%, Val Loss: 1.1779, Val Acc: 59.52%\n",
            "Epoch [41/50], Train Loss: 1.0917, Train Acc: 64.32%, Val Loss: 1.1752, Val Acc: 59.25%\n",
            "Epoch [42/50], Train Loss: 1.0809, Train Acc: 65.06%, Val Loss: 1.2469, Val Acc: 58.71%\n",
            "Epoch [43/50], Train Loss: 1.0590, Train Acc: 65.73%, Val Loss: 1.1242, Val Acc: 62.47%\n",
            "Epoch [44/50], Train Loss: 1.0541, Train Acc: 65.86%, Val Loss: 1.1234, Val Acc: 63.00%\n",
            "Epoch [45/50], Train Loss: 1.0306, Train Acc: 65.79%, Val Loss: 1.1207, Val Acc: 62.20%\n",
            "Epoch [46/50], Train Loss: 1.0191, Train Acc: 67.07%, Val Loss: 1.2776, Val Acc: 60.59%\n",
            "Epoch [47/50], Train Loss: 1.0116, Train Acc: 67.20%, Val Loss: 1.2218, Val Acc: 58.18%\n",
            "Epoch [48/50], Train Loss: 0.9908, Train Acc: 67.74%, Val Loss: 1.1159, Val Acc: 61.13%\n",
            "Epoch [49/50], Train Loss: 0.9733, Train Acc: 67.94%, Val Loss: 1.0461, Val Acc: 64.88%\n",
            "Epoch [50/50], Train Loss: 0.9761, Train Acc: 68.54%, Val Loss: 1.0724, Val Acc: 64.88%\n",
            "Finished Training\n",
            "Best model was with fold number  3 \n",
            "\n",
            "Epoch [1/50], Train Loss: 2.1923, Train Acc: 10.92%, Val Loss: 2.1852, Val Acc: 20.97%\n",
            "Epoch [2/50], Train Loss: 2.1440, Train Acc: 19.97%, Val Loss: 2.1382, Val Acc: 26.34%\n",
            "Epoch [3/50], Train Loss: 2.1034, Train Acc: 30.56%, Val Loss: 2.0985, Val Acc: 34.41%\n",
            "Epoch [4/50], Train Loss: 2.0667, Train Acc: 36.73%, Val Loss: 2.0639, Val Acc: 38.44%\n",
            "Epoch [5/50], Train Loss: 2.0330, Train Acc: 39.48%, Val Loss: 2.0320, Val Acc: 40.59%\n",
            "Epoch [6/50], Train Loss: 1.9997, Train Acc: 41.62%, Val Loss: 2.0068, Val Acc: 41.40%\n",
            "Epoch [7/50], Train Loss: 1.9712, Train Acc: 41.76%, Val Loss: 1.9774, Val Acc: 41.40%\n",
            "Epoch [8/50], Train Loss: 1.9408, Train Acc: 43.23%, Val Loss: 1.9499, Val Acc: 41.94%\n",
            "Epoch [9/50], Train Loss: 1.9112, Train Acc: 45.31%, Val Loss: 1.9242, Val Acc: 42.47%\n",
            "Epoch [10/50], Train Loss: 1.8780, Train Acc: 45.71%, Val Loss: 1.9011, Val Acc: 44.09%\n",
            "Epoch [11/50], Train Loss: 1.8489, Train Acc: 47.12%, Val Loss: 1.8695, Val Acc: 46.77%\n",
            "Epoch [12/50], Train Loss: 1.8192, Train Acc: 48.46%, Val Loss: 1.8433, Val Acc: 47.31%\n",
            "Epoch [13/50], Train Loss: 1.7859, Train Acc: 49.26%, Val Loss: 1.8064, Val Acc: 52.69%\n",
            "Epoch [14/50], Train Loss: 1.7491, Train Acc: 52.21%, Val Loss: 1.7788, Val Acc: 52.96%\n",
            "Epoch [15/50], Train Loss: 1.7240, Train Acc: 51.14%, Val Loss: 1.7504, Val Acc: 52.42%\n",
            "Epoch [16/50], Train Loss: 1.6869, Train Acc: 52.55%, Val Loss: 1.7160, Val Acc: 51.34%\n",
            "Epoch [17/50], Train Loss: 1.6551, Train Acc: 52.88%, Val Loss: 1.6924, Val Acc: 51.88%\n",
            "Epoch [18/50], Train Loss: 1.6206, Train Acc: 53.08%, Val Loss: 1.6605, Val Acc: 52.15%\n",
            "Epoch [19/50], Train Loss: 1.5862, Train Acc: 54.42%, Val Loss: 1.6327, Val Acc: 52.96%\n",
            "Epoch [20/50], Train Loss: 1.5500, Train Acc: 55.36%, Val Loss: 1.5988, Val Acc: 53.23%\n",
            "Epoch [21/50], Train Loss: 1.5189, Train Acc: 56.97%, Val Loss: 1.5562, Val Acc: 54.03%\n",
            "Epoch [22/50], Train Loss: 1.4744, Train Acc: 57.37%, Val Loss: 1.5294, Val Acc: 53.49%\n",
            "Epoch [23/50], Train Loss: 1.4543, Train Acc: 58.11%, Val Loss: 1.4924, Val Acc: 53.76%\n",
            "Epoch [24/50], Train Loss: 1.4084, Train Acc: 58.71%, Val Loss: 1.4783, Val Acc: 52.15%\n",
            "Epoch [25/50], Train Loss: 1.3834, Train Acc: 59.85%, Val Loss: 1.4247, Val Acc: 54.57%\n",
            "Epoch [26/50], Train Loss: 1.3513, Train Acc: 61.26%, Val Loss: 1.4000, Val Acc: 54.84%\n",
            "Epoch [27/50], Train Loss: 1.3317, Train Acc: 60.79%, Val Loss: 1.3654, Val Acc: 54.84%\n",
            "Epoch [28/50], Train Loss: 1.3023, Train Acc: 62.33%, Val Loss: 1.3525, Val Acc: 55.65%\n",
            "Epoch [29/50], Train Loss: 1.2780, Train Acc: 62.06%, Val Loss: 1.3282, Val Acc: 56.72%\n",
            "Epoch [30/50], Train Loss: 1.2639, Train Acc: 62.53%, Val Loss: 1.3061, Val Acc: 57.53%\n",
            "Epoch [31/50], Train Loss: 1.2325, Train Acc: 63.87%, Val Loss: 1.2851, Val Acc: 58.60%\n",
            "Epoch [32/50], Train Loss: 1.2106, Train Acc: 63.61%, Val Loss: 1.2603, Val Acc: 59.95%\n",
            "Epoch [33/50], Train Loss: 1.1908, Train Acc: 64.95%, Val Loss: 1.2597, Val Acc: 60.22%\n",
            "Epoch [34/50], Train Loss: 1.1740, Train Acc: 64.28%, Val Loss: 1.2313, Val Acc: 60.48%\n",
            "Epoch [35/50], Train Loss: 1.1580, Train Acc: 65.35%, Val Loss: 1.2100, Val Acc: 61.56%\n",
            "Epoch [36/50], Train Loss: 1.1397, Train Acc: 66.09%, Val Loss: 1.1978, Val Acc: 61.56%\n",
            "Epoch [37/50], Train Loss: 1.1282, Train Acc: 66.29%, Val Loss: 1.1976, Val Acc: 59.68%\n",
            "Epoch [38/50], Train Loss: 1.1109, Train Acc: 66.62%, Val Loss: 1.1886, Val Acc: 57.80%\n",
            "Epoch [39/50], Train Loss: 1.0907, Train Acc: 66.22%, Val Loss: 1.1494, Val Acc: 62.10%\n",
            "Epoch [40/50], Train Loss: 1.0829, Train Acc: 67.56%, Val Loss: 1.1327, Val Acc: 63.98%\n",
            "Epoch [41/50], Train Loss: 1.0638, Train Acc: 66.96%, Val Loss: 1.1219, Val Acc: 62.90%\n",
            "Epoch [42/50], Train Loss: 1.0466, Train Acc: 68.70%, Val Loss: 1.1160, Val Acc: 62.37%\n",
            "Epoch [43/50], Train Loss: 1.0258, Train Acc: 67.90%, Val Loss: 1.0769, Val Acc: 64.25%\n",
            "Epoch [44/50], Train Loss: 1.0127, Train Acc: 68.43%, Val Loss: 1.0718, Val Acc: 64.25%\n",
            "Epoch [45/50], Train Loss: 1.0001, Train Acc: 67.96%, Val Loss: 1.0675, Val Acc: 64.78%\n",
            "Epoch [46/50], Train Loss: 0.9827, Train Acc: 69.71%, Val Loss: 1.0496, Val Acc: 60.75%\n",
            "Epoch [47/50], Train Loss: 0.9694, Train Acc: 69.24%, Val Loss: 1.0240, Val Acc: 65.05%\n",
            "Epoch [48/50], Train Loss: 0.9470, Train Acc: 70.51%, Val Loss: 1.0274, Val Acc: 65.32%\n",
            "Epoch [49/50], Train Loss: 0.9420, Train Acc: 70.17%, Val Loss: 1.0076, Val Acc: 64.78%\n",
            "Epoch [50/50], Train Loss: 0.9258, Train Acc: 70.51%, Val Loss: 0.9785, Val Acc: 66.13%\n",
            "Finished Training\n",
            "Best model was with fold number  3 \n",
            "\n",
            "Done training!\n"
          ]
        }
      ],
      "source": [
        "save_dir = '/home/stefanotrenti/AML/project/AOL_test'\n",
        "name = 'AOL_function_tests.txt'\n",
        "\n",
        "optimizers = ['adam','SGD','RMSprop']\n",
        "losses = ['cross_entropy','MSELoss','BCELoss']\n",
        "epochs = 50\n",
        "\n",
        "DV_CNN_v3_relu, DV_CNN_v3_relu_models = Train_K_FOLDS(DV_CNN_v3, 50, 0.001, 5, 64, 'cross_entropy', 'SGD')\n",
        "\n",
        "print('Done training!')\n",
        "\n",
        "modelName_relu = DV_CNN_v3_relu.get_architecture_name() + ' 50 epochs'\n",
        "\n",
        "saveResults(DV_CNN_v3_relu,save_dir,name,modelName_relu, 'cross_entropy', 'SGD')\n",
        "\n",
        "models_for_voting_ALL = []\n",
        "\n",
        "models_for_voting_ALL.extend(DV_CNN_v3_relu_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EVALUATE NETWORK ON GENERATED TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[222,   0,   0,   2,   0,   0,   0,   1,   8],\n",
              "        [ 55,   1,  13,   0,   0,   0,   0,   0,   6],\n",
              "        [  0,   0, 256,   0,   0,   0,   0,   0,   0],\n",
              "        [  4,   0,   0, 123,   0,   0,   0,  20,   0],\n",
              "        [  5,   0,   0,  32,   0,   0,   1,  27,   0],\n",
              "        [  9,   0,   4,  31,   0,   0,   2,  48,   0],\n",
              "        [  1,   0,   1,  24,   0,   0,   5,  36,   0],\n",
              "        [  8,   1,   0,  58,   0,   0,   0,  72,   0],\n",
              "        [  1,   0,   0,   0,   0,   0,   0,   0, 167]]),\n",
              " 0.6079835110153993,\n",
              " 0.4877242025568614)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_network(DV_CNN_v3_relu, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LOAD TEST FOLDER DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def get_int(self, text):\n",
        "        return [int(c) if c.isdigit() else c for c in re.split('(\\d+)', text)]\n",
        "\n",
        "    def __init__(self, images_folder, transform=None):\n",
        "        self.images_folder = images_folder\n",
        "        self.image_files = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]\n",
        "        self.image_files.sort(key=self.get_int)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.images_folder, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "    \n",
        "    \n",
        "'''   \n",
        "    \n",
        "#run if we want to use new test folder every time we use csv\n",
        "\n",
        "transform_test = transforms.Compose([removeBackground,input_transform])\n",
        "\n",
        "inference_dataset = TestDataset(images_folder=test_dataset_path, transform=transform_test)\n",
        "\n",
        "test_dataset_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "'''\n",
        "\n",
        "#IF YOU WANT TO TAKE IMAGES FROM THE TEST FLDER WITH NO BG\n",
        "\n",
        "test_nobg_path = 'data-students/TEST-NOBG'\n",
        "\n",
        "inference_dataset = TestDataset(images_folder=test_nobg_path, transform=input_transform)\n",
        "\n",
        "test_dataset_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CREATE AND SAVE CSV FILE WITH PREDICTION RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score:  0.6079835110153993\n",
            "B_acc:  0.4877242025568614\n",
            "CSV file created successfully.\n",
            "CSV file created successfully.\n"
          ]
        }
      ],
      "source": [
        "createCSV(DV_CNN_v3_relu, test_dataset_loader,\"\",\"DV_CNN_v3_relu.csv\")\n",
        "\n",
        "#createCSV(models_for_voting_best, test_dataset_loader,\"voting\",\"predictions_CNN_voting_best.csv\")\n",
        "createCSV(models_for_voting_ALL, test_dataset_loader,\"voting\",\"predictions_CNN_voting_ALL.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## END"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
